---

layout: post
title: 切片逆回归(SIR)--R realization
categories: dimension_reduction

---

# 切片逆回归(SIR)--R realization

Author: Delin Zhao

---

One important condition the distribution of $$\mathbf{x}$$ should satisfy to use **sliced inverse regression(SIR)**:

For any b in $$\mathbf{R}^{p}$$, the conditional expectation $$E(b\mathbf{x}|\beta_{1}\mathbf{x},...\beta_{K}\mathbf{x})$$ is linear in $$\beta_{1}\mathbf{x},...,\beta_{K}\mathbf{x}$$, that is, $$\exists$$ some constant $$c_{0},...,c_{K}$$, E(b\mathbf{x}|\beta_{1}\mathbf{x},...\beta_{K}\mathbf{x})=c_{0}+c_{1}\beta_{1}\mathbf{x}+...+c_{K}\beta_{K}\mathbf{x}.

This condition will be statisfied when the distribution of $$\mathbf{x}$$ is elliptically symmetric, and fortunatelly, the normal distribution satisfies this condition.

---

{% highlight R %}
SIR <- function(y,x,H,K){
  sigma <- cov(x)
  xbar <- colMeans(x)
  trans <- sqrtm(solve(sigma))
  x_new <- t(apply(x,1,function(x) trans%*%(x-xbar)))
  n <- length(y)
  slice <- quantile(y,seq(0,1,by = 1/H))
  belong <- apply(as.matrix(y), 1, function(x){
    sum(x>=slice)
  })
  belong[which(y==max(y))] <- H
  P <- apply(v <- matrix(1:H),1,function(x){
    length(which(belong==x))/n
  })
  sample_mean <- t(apply(v,1,function(x){
    apply(x_new[which(belong==x),],2,mean)
  }))
  V <- t(sample_mean)%*%diag(P)%*%sample_mean
  eigen_decom <- eigen(V)
  eigen_vector <- eigen_decom$vectors
  eta <- eigen_vector[,1:K]
  beta <- t(eta)%*%trans
  beta_S <- t(apply(beta,1,function(x) x/sqrt(sum(x^2))))
  return(beta_S)
}
{% endhighlight %}

---

Also we can use the function dr in "dr" package, the difference lies in the way of determining the number of y's in each slice.

---
## Reference

Ker-Chau Li(1991),"Sliced Inverse Regression for Dimension Reduction" *Journal of the American Statistical Association*, Vol. 86, No. 414 (Jun., 1991), pp.316-327.
